{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75391d35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T22:20:43.754029Z",
     "iopub.status.busy": "2025-05-13T22:20:43.753749Z",
     "iopub.status.idle": "2025-05-13T22:20:57.448020Z",
     "shell.execute_reply": "2025-05-13T22:20:57.447451Z",
     "shell.execute_reply.started": "2025-05-13T22:20:43.753982Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.layers import Dense \n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e398a91",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5565a29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T22:20:57.449825Z",
     "iopub.status.busy": "2025-05-13T22:20:57.449260Z",
     "iopub.status.idle": "2025-05-13T22:20:57.453042Z",
     "shell.execute_reply": "2025-05-13T22:20:57.452265Z",
     "shell.execute_reply.started": "2025-05-13T22:20:57.449805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "path = \"../speech_emotion_dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a73af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T22:20:57.454572Z",
     "iopub.status.busy": "2025-05-13T22:20:57.454310Z",
     "iopub.status.idle": "2025-05-13T22:20:57.504218Z",
     "shell.execute_reply": "2025-05-13T22:20:57.503533Z",
     "shell.execute_reply.started": "2025-05-13T22:20:57.454548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_and_listen(audio_path, sample_rate=22050):\n",
    "    # Load the audio\n",
    "    y, sr = librosa.load(audio_path, sr=sample_rate)\n",
    "\n",
    "    # Plot the waveform\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title(f\"Waveform of {audio_path}\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Play the audio\n",
    "    return Audio(y, rate=sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1751fb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T22:20:57.505979Z",
     "iopub.status.busy": "2025-05-13T22:20:57.505763Z",
     "iopub.status.idle": "2025-05-13T22:21:11.329803Z",
     "shell.execute_reply": "2025-05-13T22:21:11.329058Z",
     "shell.execute_reply.started": "2025-05-13T22:20:57.505962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Locate the folder with .wav files\n",
    "audio_dir = os.path.join(path, \"Crema\")  # Make sure this folder name matches what you see\n",
    "\n",
    "# List all .wav files\n",
    "audio_files = [f for f in os.listdir(audio_dir) if f.endswith(\".wav\")]\n",
    "print(f\"Total audio files: {len(audio_files)}\")\n",
    "\n",
    "# Mapping of emotion codes to labels\n",
    "emotion_map = {\n",
    "    \"ANG\": \"Angry\",\n",
    "    \"DIS\": \"Disgust\",\n",
    "    \"FEA\": \"Fear\",\n",
    "    \"HAP\": \"Happy\",\n",
    "    \"NEU\": \"Neutral\",\n",
    "    \"SAD\": \"Sad\"\n",
    "}\n",
    "\n",
    "# Extract a sample of one file per emotion\n",
    "samples = {}\n",
    "for file in audio_files:\n",
    "    parts = file.split('_')\n",
    "    emotion_code = parts[2]\n",
    "    if emotion_code in emotion_map and emotion_code not in samples:\n",
    "        samples[emotion_code] = file\n",
    "\n",
    "# Plot and listen to one file per emotion\n",
    "for code, filename in samples.items():\n",
    "    filepath = os.path.join(audio_dir, filename)\n",
    "    signal, sr = librosa.load(filepath, sr=None)\n",
    "    \n",
    "    # Plot waveform\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    librosa.display.waveshow(signal, sr=sr)\n",
    "    plt.title(f\"{emotion_map[code]} - {filename}\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Play audio (if running in Jupyter Notebook)\n",
    "    try:\n",
    "        from IPython.display import Audio, display\n",
    "        display(Audio(filepath))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c5aaca",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600091d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T22:21:11.331104Z",
     "iopub.status.busy": "2025-05-13T22:21:11.330596Z",
     "iopub.status.idle": "2025-05-13T22:21:11.337489Z",
     "shell.execute_reply": "2025-05-13T22:21:11.336868Z",
     "shell.execute_reply.started": "2025-05-13T22:21:11.331084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_noise(data, random=False, rate=0.035, threshold=0.075):\n",
    "    if random:\n",
    "        rate = np.random.random() * threshold\n",
    "    # noise = rate * np.random.uniform() * np.amax(data)\n",
    "    noise = rate * np.amax(data) * np.random.normal(0, 1, size=data.shape)\n",
    "    augmented_data = data + noise * np.random.normal(size=data.shape[0])\n",
    "    return augmented_data\n",
    "\n",
    "def shifting(data, rate=1000, wrap=False):\n",
    "    shift_amount = int(np.random.uniform(low=-5, high=5) * rate)\n",
    "    if wrap:\n",
    "        return np.roll(data, shift_amount)\n",
    "    else:\n",
    "        if shift_amount > 0:\n",
    "            return np.concatenate((np.zeros(shift_amount), data[:-shift_amount]))\n",
    "        else:\n",
    "            return np.concatenate((data[-shift_amount:], np.zeros(-shift_amount)))\n",
    "\n",
    "\n",
    "def pitching(y, sr, n_steps=4):\n",
    "    y_shifted = librosa.effects.pitch_shift(y, sr=sr, n_steps=n_steps)\n",
    "    return y_shifted\n",
    "\n",
    "def stretching(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(data, rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c90cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T22:21:11.338681Z",
     "iopub.status.busy": "2025-05-13T22:21:11.338360Z",
     "iopub.status.idle": "2025-05-13T22:21:11.357635Z",
     "shell.execute_reply": "2025-05-13T22:21:11.357032Z",
     "shell.execute_reply.started": "2025-05-13T22:21:11.338657Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_features(filepath, sr=16000, frame_length=2048, hop_length=512, n_mels=128, augment = False):\n",
    "    # Load audio\n",
    "    signal, sr = librosa.load(filepath, sr=sr)  # Use desired sampling rate\n",
    "    # Trim silent edges\n",
    "    signal, _ = librosa.effects.trim(signal)\n",
    "    \n",
    "    if augment:\n",
    "        # Randomly choose one augmentation (or apply all if desired)\n",
    "        aug_type = np.random.choice(['noise', 'shift', 'pitch', 'stretch'])\n",
    "        if aug_type == 'noise':\n",
    "            signal = add_noise(signal, random=True)\n",
    "        elif aug_type == 'shift':\n",
    "            signal = shifting(signal)\n",
    "        elif aug_type == 'pitch':\n",
    "            signal = pitching(signal, sr)\n",
    "        elif aug_type == 'stretch':\n",
    "            try:\n",
    "                signal = stretching(signal, rate=np.random.uniform(0.8, 1.2))\n",
    "            except:\n",
    "                pass  # stretching can sometimes result in shape mismatch    \n",
    "\n",
    "    # --- Zero Crossing Rate ---\n",
    "    zcr = librosa.feature.zero_crossing_rate(\n",
    "        y=signal, frame_length=frame_length, hop_length=hop_length\n",
    "    )[0]  # shape: (frames,)\n",
    "\n",
    "    # --- Energy (normalized) ---\n",
    "    energy = np.array([\n",
    "        np.sum(signal[i:i+frame_length]**2) / frame_length\n",
    "        for i in range(0, len(signal) - frame_length + 1, hop_length)\n",
    "    ])\n",
    "    \n",
    "    # --- Mel Spectrogram ---\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=signal, sr=sr, n_mels=n_mels,\n",
    "        n_fft=frame_length, hop_length=hop_length\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    return zcr, energy, mel_spec_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb45a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T22:21:11.358469Z",
     "iopub.status.busy": "2025-05-13T22:21:11.358267Z",
     "iopub.status.idle": "2025-05-13T22:21:13.676451Z",
     "shell.execute_reply": "2025-05-13T22:21:13.675630Z",
     "shell.execute_reply.started": "2025-05-13T22:21:11.358453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test on a sample\n",
    "test_file = os.path.join(audio_dir, random.choice(audio_files))\n",
    "zcr, energy, mel_spec = extract_features(test_file, augment=True)\n",
    "fs1 = zcr , energy\n",
    "fs2 = mel_spec\n",
    "# Plot features\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(zcr)\n",
    "plt.title(\"Zero Crossing Rate\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(energy)\n",
    "plt.title(\"Energy\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "librosa.display.specshow(mel_spec, sr=16000, x_axis='time', y_axis='mel')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title(\"Mel Spectrogram\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a3d773",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T22:21:13.677791Z",
     "iopub.status.busy": "2025-05-13T22:21:13.677280Z",
     "iopub.status.idle": "2025-05-13T22:25:37.230618Z",
     "shell.execute_reply": "2025-05-13T22:25:37.229859Z",
     "shell.execute_reply.started": "2025-05-13T22:21:13.677762Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare containers\n",
    "zcr_list = []\n",
    "energy_list = []\n",
    "mel_spec_list = []\n",
    "labels = []\n",
    "\n",
    "valid_emotions = {\"ANG\", \"DIS\", \"FEA\", \"HAP\", \"NEU\", \"SAD\"}\n",
    "\n",
    "# Loop through files\n",
    "for file in tqdm(audio_files):\n",
    "    parts = file.split('_')\n",
    "    emotion_code = parts[2]\n",
    "\n",
    "    if emotion_code not in valid_emotions:\n",
    "        continue \n",
    "\n",
    "    filepath = os.path.join(audio_dir, file)\n",
    "    \n",
    "    try:\n",
    "        # Original (non-augmented) data\n",
    "        zcr, energy, mel_spec = extract_features(filepath, augment=False)\n",
    "        zcr_list.append(zcr)\n",
    "        energy_list.append(energy)\n",
    "        mel_spec_list.append(mel_spec)\n",
    "        labels.append(emotion_code)\n",
    "\n",
    "        # Augmented version (1 sample per original only for 1D features)\n",
    "        zcr_aug, energy_aug, mel_spec_aug = extract_features(filepath, augment=True)\n",
    "        zcr_list.append(zcr_aug)\n",
    "        energy_list.append(energy_aug)\n",
    "        mel_spec_list.append(mel_spec)\n",
    "        labels.append(emotion_code)  # Same label\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed for {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44befa75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T22:25:37.231939Z",
     "iopub.status.busy": "2025-05-13T22:25:37.231665Z",
     "iopub.status.idle": "2025-05-13T22:25:37.259320Z",
     "shell.execute_reply": "2025-05-13T22:25:37.258552Z",
     "shell.execute_reply.started": "2025-05-13T22:25:37.231904Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X =zcr_list, energy_list, mel_spec_list\n",
    "print(len(zcr_list), len(energy_list), len(mel_spec_list))\n",
    "\n",
    "le = LabelEncoder()\n",
    "label_array = le.fit_transform(labels)\n",
    "\n",
    "# Get mapping\n",
    "label_map = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Label mapping:\", label_map)\n",
    "label_array.shape\n",
    "print(label_array.shape)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'zcr': zcr_list,\n",
    "    'energy': energy_list,\n",
    "    'mel_spec': mel_spec_list,\n",
    "    'label': label_array\n",
    "})\n",
    "\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "\n",
    "print(\"Shape of features:\", X.shape)\n",
    "print(\"Shape of labels:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6685124",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff146e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T22:25:37.261842Z",
     "iopub.status.busy": "2025-05-13T22:25:37.261642Z",
     "iopub.status.idle": "2025-05-13T22:25:37.279822Z",
     "shell.execute_reply": "2025-05-13T22:25:37.279254Z",
     "shell.execute_reply.started": "2025-05-13T22:25:37.261826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Train+Val (70%) and Test (30%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. From Train+Val, get 5% as validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.05,  # 5% of 70% ≈ 3.5% of total data\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Show shapes\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Validation shape:\", X_val.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca17950",
   "metadata": {},
   "source": [
    "### Create the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dc8994",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T22:25:37.280761Z",
     "iopub.status.busy": "2025-05-13T22:25:37.280505Z",
     "iopub.status.idle": "2025-05-13T22:25:37.289094Z",
     "shell.execute_reply": "2025-05-13T22:25:37.288442Z",
     "shell.execute_reply.started": "2025-05-13T22:25:37.280723Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"First feature Space\"\"\" \n",
    "X_train1 = X_train[X_train.columns[0:2]]\n",
    "X_val1   = X_val[X_val.columns[0:2]]\n",
    "X_test1  = X_test[X_test.columns[0:2]]\n",
    "\n",
    "\"\"\"Second feature space\"\"\"\n",
    "X_train2 = X_train[X_train.columns[2:]]\n",
    "X_val2   = X_val[X_val.columns[2:]]\n",
    "X_test2  = X_test[X_test.columns[2:]]\n",
    "\n",
    "print(\"Train shape1:\", X_train1.shape)\n",
    "print(\"Validation shape1:\", X_val1.shape)\n",
    "print(\"Test shape1:\", X_test1.shape)\n",
    "\n",
    "print(\"Train shape2:\", X_train2.shape)\n",
    "print(\"Validation shape2:\", X_val2.shape)\n",
    "print(\"Test shape2:\", X_test2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81671c61",
   "metadata": {},
   "source": [
    "### Prepare Data for Model Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47113b8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T22:25:37.290037Z",
     "iopub.status.busy": "2025-05-13T22:25:37.289760Z",
     "iopub.status.idle": "2025-05-13T22:25:42.873386Z",
     "shell.execute_reply": "2025-05-13T22:25:42.872593Z",
     "shell.execute_reply.started": "2025-05-13T22:25:37.289996Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad 1D features\n",
    "def stack_and_pad_1d(features1, features2, max_len=400):\n",
    "    x1 = pad_sequences(features1, maxlen=max_len, padding='post', dtype='float32')\n",
    "    x2 = pad_sequences(features2, maxlen=max_len, padding='post', dtype='float32')\n",
    "    return np.concatenate([x1, x2], axis=-1)\n",
    "\n",
    "X_train1_padded = stack_and_pad_1d(X_train1['zcr'], X_train1['energy'])\n",
    "X_val1_padded = stack_and_pad_1d(X_val1['zcr'], X_val1['energy'])\n",
    "X_test1_padded = stack_and_pad_1d(X_test1['zcr'], X_test1['energy'])\n",
    "\n",
    "# Pad 2D mel spectrograms\n",
    "def pad_melspec_2d(mels, max_time=400, max_freq=None):\n",
    "    padded = []\n",
    "    for m in mels:\n",
    "        # Determine the frequency dimension size if not provided\n",
    "        if max_freq is None:\n",
    "            max_freq = m.shape[0]\n",
    "        \n",
    "        # Pad time dimension\n",
    "        if m.shape[1] < max_time:\n",
    "            pad_width_time = max_time - m.shape[1]\n",
    "            m_padded = np.pad(m, ((0, 0), (0, pad_width_time)), mode='constant')\n",
    "        else:\n",
    "            m_padded = m[:, :max_time]\n",
    "        \n",
    "        # Pad frequency dimension if necessary\n",
    "        if m.shape[0] < max_freq:\n",
    "            pad_width_freq = max_freq - m.shape[0]\n",
    "            m_padded = np.pad(m_padded, ((0, pad_width_freq), (0, 0)), mode='constant')\n",
    "        else:\n",
    "            m_padded = m_padded[:max_freq, :]\n",
    "        \n",
    "        padded.append(m_padded)\n",
    "    \n",
    "    return np.array(padded)[..., np.newaxis]  # Add channel dimension for 2D CNNs\n",
    "\n",
    "# Apply the function to pad mel spectrograms\n",
    "X_train2_padded = pad_melspec_2d(X_train2['mel_spec'])\n",
    "X_val2_padded = pad_melspec_2d(X_val2['mel_spec'])\n",
    "X_test2_padded = pad_melspec_2d(X_test2['mel_spec'])\n",
    "\n",
    "# Convert labels\n",
    "y_train_arr = tf.keras.utils.to_categorical(y_train)\n",
    "y_val_arr = tf.keras.utils.to_categorical(y_val)\n",
    "y_test_arr = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(\"Train shape:\", X_train1_padded.shape)\n",
    "print(\"Validation shape:\", X_val1_padded.shape)\n",
    "print(\"Test shape:\", X_test1_padded.shape)\n",
    "\n",
    "print(\"Train 2D shape:\", X_train2_padded.shape)\n",
    "print(\"Validation 2D shape:\", X_val2_padded.shape)\n",
    "print(\"Test 2D shape:\", X_test2_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d30e3d",
   "metadata": {},
   "source": [
    "## Define CNN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b2341f",
   "metadata": {},
   "source": [
    "### 1D CNN (ZCR + Energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa484726-d5e4-4bc3-93fe-b90d5a39174c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T22:25:42.882564Z",
     "iopub.status.busy": "2025-05-13T22:25:42.882340Z",
     "iopub.status.idle": "2025-05-13T22:25:42.904242Z",
     "shell.execute_reply": "2025-05-13T22:25:42.903628Z",
     "shell.execute_reply.started": "2025-05-13T22:25:42.882542Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_1d_cnn():\n",
    "    inputs = tf.keras.Input(shape=(800, 1))\n",
    "\n",
    "    x = L.Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu')(inputs)\n",
    "    x = L.MaxPool1D(pool_size=3, strides=2, padding='same')(x)\n",
    "\n",
    "    # Block 1\n",
    "    x = L.Conv1D(512, kernel_size=5, padding='same', activation='relu')(x)\n",
    "    x = L.Conv1D(512, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    x = L.MaxPool1D(pool_size=3, strides=2, padding='same')(x)\n",
    "    x = L.Dropout(0.3)(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = L.Conv1D(256, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    x = L.Conv1D(256, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    x = L.MaxPool1D(pool_size=3, strides=2, padding='same')(x)\n",
    "    x = L.Dropout(0.3)(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = L.Conv1D(128, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    x = L.MaxPool1D(pool_size=2, strides=2, padding='same')(x)\n",
    "\n",
    "    # Final Layers\n",
    "    x = L.GlobalAveragePooling1D()(x)  # Less prone to overfitting than Flatten\n",
    "    x = L.Dense(512, activation='relu')(x)\n",
    "    outputs = L.Dense(6, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be66e829",
   "metadata": {},
   "source": [
    "### 2D CNN (Mel Spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8020c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T22:25:42.905210Z",
     "iopub.status.busy": "2025-05-13T22:25:42.904932Z",
     "iopub.status.idle": "2025-05-13T22:25:42.921187Z",
     "shell.execute_reply": "2025-05-13T22:25:42.920625Z",
     "shell.execute_reply.started": "2025-05-13T22:25:42.905186Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_2d_cnn():\n",
    "    inputs = tf.keras.Input(shape=(128, 400, 1))  # Mel-spectrogram shape\n",
    "\n",
    "    x = L.Conv2D(512, kernel_size=(5, 5), strides=(1, 1), padding='same', activation='relu')(inputs)\n",
    "    x = L.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    # Block 1\n",
    "    x = L.Conv2D(512, kernel_size=(5, 5), padding='same', activation='relu')(x)\n",
    "    x = L.Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
    "    x = L.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = L.Dropout(0.3)(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = L.Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
    "    x = L.Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
    "    x = L.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = L.Dropout(0.3)(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = L.Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
    "    x = L.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    # Final Layers\n",
    "    x = L.GlobalAveragePooling2D()(x)\n",
    "    x = L.Dense(512, activation='relu')(x)\n",
    "    outputs = L.Dense(6, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435812d",
   "metadata": {},
   "source": [
    "### Train the 1D CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e200f9d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T22:25:42.922561Z",
     "iopub.status.busy": "2025-05-13T22:25:42.921965Z",
     "iopub.status.idle": "2025-05-13T22:25:42.938731Z",
     "shell.execute_reply": "2025-05-13T22:25:42.938182Z",
     "shell.execute_reply.started": "2025-05-13T22:25:42.922542Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor= 'val_accuracy', patience=10, restore_best_weights=True, mode ='auto'),\n",
    "    ReduceLROnPlateau(monitor='val_accuracy',patience=5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f26bc7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T22:25:42.939698Z",
     "iopub.status.busy": "2025-05-13T22:25:42.939412Z",
     "iopub.status.idle": "2025-05-13T22:25:43.134843Z",
     "shell.execute_reply": "2025-05-13T22:25:43.134058Z",
     "shell.execute_reply.started": "2025-05-13T22:25:42.939682Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train1_scaled = scaler.fit_transform(X_train1_padded)\n",
    "X_val1_scaled = scaler.transform(X_val1_padded)\n",
    "X_test1_scaled = scaler.transform(X_test1_padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d00a85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T22:26:03.075838Z",
     "iopub.status.busy": "2025-05-13T22:26:03.075571Z",
     "iopub.status.idle": "2025-05-13T22:35:39.506432Z",
     "shell.execute_reply": "2025-05-13T22:35:39.505801Z",
     "shell.execute_reply.started": "2025-05-13T22:26:03.075819Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train 1D CNN\n",
    "model_1d = build_1d_cnn()\n",
    "model_1d.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_1d.summary()\n",
    "# history_1D = model_1d.fit(X_train1_scaled, y_train_arr, epochs=100, batch_size=128, \n",
    "#              validation_data=(X_val1_scaled, y_val_arr), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169a2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(\"C:\\Users\\Kimo Store\\Desktop\\Term 8\\Pattern\\Labs\\Lab 3\\Speech-Emotion-Recognition\\model_1d.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb4f73",
   "metadata": {},
   "source": [
    "### Train the 2D CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9409b570",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-13T22:25:45.766394Z",
     "iopub.status.idle": "2025-05-13T22:25:45.766595Z",
     "shell.execute_reply": "2025-05-13T22:25:45.766505Z",
     "shell.execute_reply.started": "2025-05-13T22:25:45.766496Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Train 2D CNN\n",
    "model_2d = build_2d_cnn()\n",
    "model_2d.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_2d.summary()\n",
    "# history_2D = model_2d.fit(X_train2_padded, y_train_arr, epochs=1, batch_size=32, \n",
    "#              validation_data=(X_val2_padded, y_val_arr), callbacks=callbacks)\n",
    "\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size=32):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start = index * self.batch_size\n",
    "        end = start + self.batch_size\n",
    "        return self.X[start:end], self.y[start:end]\n",
    "\n",
    "train_gen = DataGenerator(X_train2_padded, y_train_arr)\n",
    "val_gen = DataGenerator(X_val2_padded, y_val_arr)\n",
    "\n",
    "\n",
    "history = model_2d.fit(train_gen, validation_data=val_gen, epochs=30, callbacks=callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6147b2",
   "metadata": {},
   "source": [
    "### 1D Model Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158829d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T22:35:55.180649Z",
     "iopub.status.busy": "2025-05-13T22:35:55.179926Z",
     "iopub.status.idle": "2025-05-13T22:35:55.529763Z",
     "shell.execute_reply": "2025-05-13T22:35:55.528957Z",
     "shell.execute_reply.started": "2025-05-13T22:35:55.180623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train accuracy\n",
    "train_acc_1d = history_1D.history['accuracy']\n",
    "# validation accuracy\n",
    "val_acc_1d = history_1D.history['val_accuracy']\n",
    "# train loss\n",
    "train_loss_1d = history_1D.history['loss']\n",
    "# validation loss\n",
    "val_loss_1d = history_1D.history['val_loss']\n",
    "\n",
    "print(\"Train accuracy 1D:\", train_acc_1d)\n",
    "print(\"Validation accuracy 1D:\", val_acc_1d)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc_1d, label='Train 1D')\n",
    "plt.plot(val_acc_1d, label='Validation 1D')\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss_1d, label='Train 1D')\n",
    "plt.plot(val_loss_1d, label='Validation 1D')\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab92d76a",
   "metadata": {},
   "source": [
    "### 2D Model Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58739670",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train accuracy\n",
    "train_acc_2d = history_2D.history['accuracy']\n",
    "# validation accuracy\n",
    "val_acc_2d = history_2D.history['val_accuracy']\n",
    "# train loss\n",
    "train_loss_2d = history_2D.history['loss']\n",
    "# validation loss\n",
    "val_loss_2d = history_2D.history['val_loss']\n",
    "\n",
    "print(\"Train accuracy 2D:\", train_acc_2d)\n",
    "print(\"Validation accuracy 2D:\", val_acc_2d)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc_2d, label='Train 2D')\n",
    "plt.plot(val_acc_2d, label='Validation 2D')\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss_2d, label='Train 2D')\n",
    "plt.plot(val_loss_2d, label='Validation 2D')\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac52736",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7bd57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss_1d, test_acc_1d = model_1d.evaluate(X_test1_scaled, y_test_arr)\n",
    "print(f\"Test accuracy 1D: {test_acc_1d:.4f}\")\n",
    "\n",
    "#f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "y_pred_1d = model_1d.predict(X_test1_scaled)\n",
    "y_pred_1d_classes = np.argmax(y_pred_1d, axis=1)\n",
    "f1_1d = f1_score(y_test, y_pred_1d_classes, average='weighted')\n",
    "print(f\"F1 Score 1D: {f1_1d:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss_2d, test_acc_2d = model_2d.evaluate(X_test2_padded, y_test_arr)\n",
    "print(f\"Test accuracy 2D: {test_acc_2d:.4f}\")\n",
    "\n",
    "#f1 score\n",
    "y_pred_2d = model_2d.predict(X_test2_padded)\n",
    "y_pred_2d_classes = np.argmax(y_pred_2d, axis=1)\n",
    "f1_2d = f1_score(y_test, y_pred_2d_classes, average='weighted')\n",
    "print(f\"F1 Score 2D: {f1_2d:.4f}\")\n",
    "\n",
    "# Save the models\n",
    "model_1d.save(\"C:\\Users\\Kimo Store\\Desktop\\Term 8\\Pattern\\Labs\\Lab 3\\Speech-Emotion-Recognition\\model_1d.h5\")\n",
    "model_2d.save(\"C:\\Users\\Kimo Store\\Desktop\\Term 8\\Pattern\\Labs\\Lab 3\\Speech-Emotion-Recognition\\model_2d.h5\")\n",
    "# Load the models\n",
    "loaded_model = tf.keras.models.load_model(\"C:\\Users\\Kimo Store\\Desktop\\Term 8\\Pattern\\Labs\\Lab 3\\Speech-Emotion-Recognition\\model_1d.h5\")\n",
    "loaded_model2 = tf.keras.models.load_model(\"C:\\Users\\Kimo Store\\Desktop\\Term 8\\Pattern\\Labs\\Lab 3\\Speech-Emotion-Recognition\\model_2d.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411170e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Confusion matrix for 1D model\n",
    "conf_matrix_1d = confusion_matrix(y_test, y_pred_1d_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_1d, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(\"Confusion Matrix - 1D Model\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix for 2D model\n",
    "conf_matrix_2d = confusion_matrix(y_test, y_pred_2d_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_2d, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(\"Confusion Matrix - 2D Model\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Find the most confusing classes for 1D model\n",
    "confusion_1d = conf_matrix_1d.copy()\n",
    "np.fill_diagonal(confusion_1d, 0)  # Ignore diagonal (correct predictions)\n",
    "most_confusing_1d = np.unravel_index(np.argmax(confusion_1d), confusion_1d.shape)\n",
    "print(f\"Most confusing classes for 1D model: {le.classes_[most_confusing_1d[0]]} and {le.classes_[most_confusing_1d[1]]}\")\n",
    "\n",
    "# Find the most confusing classes for 2D model\n",
    "confusion_2d = conf_matrix_2d.copy()\n",
    "np.fill_diagonal(confusion_2d, 0)  # Ignore diagonal (correct predictions)\n",
    "most_confusing_2d = np.unravel_index(np.argmax(confusion_2d), confusion_2d.shape)\n",
    "print(f\"Most confusing classes for 2D model: {le.classes_[most_confusing_2d[0]]} and {le.classes_[most_confusing_2d[1]]}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1118008,
     "sourceId": 1877714,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "pr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
